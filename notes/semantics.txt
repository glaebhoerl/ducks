effect-typed data for mutability polymorphism
  syntactically effect arguments are adjectives preceding the type name, e.g. `pure HashMap`, `mut HashMap`
  read-only methods are effect-polymorphic, mutating methods require a `mut HashMap`
  this works out nicely: read-only code can be shared between pure and effectful computations without annotation
    (effect-polymorphism is the default)
  but how does the type system know, or how does HashMap declare that it _has_ an effect parameter?
  does _every_ type have an effect parameter?
    both `Int` and `mut Int`?
    spider sense says this is probably wrong, will probably realize why
  this _does_ mean we can have both pure `Array(T)` and `mut Array(T)` and have the latter be efficient and indirection-free...?
    fn at(Array(T)) -> T*
      -> fn at(const Array(T)) -> const Ref(T) (`const T*` seems ambiguous: `const T` or `const *`?)
         fn at(mut Array(T)) -> mut Ref(T)
      is it "obvious" that effect polymorphism should work this way?
    perhaps `arr[i]` should translate to `*arr.at(i)` and `arr[i] = x` to `*arr.at(i) = x`
      should `&arr[i]: T*` work?
      could/should we even have an `&` operator?
  does this also let us have records with unboxed mutable fields..?
    it works for arrays and hashmaps because those are always represented as a pointer
    but if we want to pass/store records by-value, that won't really work
    is there a different/better solution? (is it important?)
    what if we give up on the unboxed representatation? would it work?
    what if specialize based on effect types too?
      we want something like this for memoizing const fns but not other fns anyways...
      is there any reason this couldn't be possible? (are effects different from types in some critical way?)
      what would the performance drawbacks be? more expensive freeze/thaw?
    does it make any sense to have Ref(record) -> Ref(record.field)?
      in this case the record is in a fixed place in memory and only has references to it passed around... and the same is true of the field?
      writing through the outer reference will also overwrite the contents of the inner reference
      reading/writing through the outer ref seems equivalent to reading/writing through each inner ref separately
      seems like the same kind of thing as CellRef in Rust...
      would be really nice if this could work! but does it really??
        (have been fooled by seductive Rustic intuitions so many times...)
      what does the type system here actually look like? would this be "primitive" somehow, or can we express it internally?
        one thing we definitely need to outlaw is `(T: Type, foo: T)` -> `Ref(Type)`...
      this seems to work as long as the memory representation of the record is flat...?
        what is the precise requirement?
        if we were to store record fields behind pointers, would it actually stop working...?
        if we wish to represent very-big record types as pointers instead, does that conflict?
        what about pointers in recursive types?
          infinite linear types `let Stream(T) = (head: T, tail: Stream(T))` are impossible to construct
          you can break it up with a function, a sum type, or a mutable reference
            in that case the subfield-reference-taking also has to stop at that boundary, so it's OK?
        what about pointers in existential types?
      this should probably also inform the treatment of mutable function-local variables, given that we're aiming for stack frame == record
        but not sure how you'd have separate immutable and mutable variables in that way...
        (adding an effect type to every record field feels heavy-handed)
        to what extent is this a syntax sugar question, and to what extent a semantics question?
  OK, so we have runtime-fixed-length arrays with (a) mutable and (b) immutable elements
    what about mutable elements and mutable length (Vec), or even immutable elements and mutable length?
    or rather, it's not just mutable length but being able to insert/remove elements at any point
      though this can be seen as just pushing/popping and shifting-by-assignment
  what about things like `io Array(T)`? do we want that? can we have it?
    how do we avoid memory-unsafe data races esp. with unboxed elements?
      we probably don't want it, because of data races, even special `atomic` methods doesn't help because `read` is polymorphic
        (the issue is not that you could const-read from an atomic array, but that `read`, even instatiated to `atomic`, doesn't use atomic operations)
        (this is assuming the behavior/code for read would be uniform across effects)
      atomics could only work on `Ref(T)` because/if it's guaranteed to always have the same size
        that's if we (perhaps intentionally) don't specialize the representation of `const Ref(T)` to just be `T`
        and anything else that's also word-sized, but how does that work? esp without typeclasses?
    what about `atomic Array(T)`?
      also, what about `atomic Table`?
      if we want normal RDBMS like stuff with transactions just being STM transactions, we want either `atomic Table` or `AtomicTable` (a.k.a. `Table`, but it's always atomic)
        the question is whether we also want `const Table`, `mut Table`, and so on, and whether that makes sense / works out (see above)
      how does this work together with `mmap` for on-disk databases?
        for remote dbs, send over a closure instead?
        cf. Unison's ideas...
    can we conceivably have a `Mutex(T)` type like in Rust in `io` with local `mut` borrowing?
      would this make sense?
        io fn withLock(type T, type Result, mutex: Mutex(T), op: fn(contents: mut Ref(T)) -> Result) -> Result
          how do we ensure that:
           * `op` is taken to be a `mut fn` which mutates only(?) `contents`
           * `Result` may not contain `contents`
           * `withLock` itself is *not* taken to be a `mut fn`?
          also, how much of this is stuff we could theoretically infer if we wanted to?
           we probably want the `mut` there to be explicit
             what's the broader policy? non-polymorphic effects should be explicit?
      what else is there: RWLocks, Chans, ...?
      (though our larger plan is STM or concurrent revisions...?)
    what about `io File` and (scoped) `mut File` to solve the resource management problem?
      is it a bad idea to tie scoping to mutability?
      actually, `mut File` is nonsense because it's impure even if scoped.
      is `mut io File` also nonsense? here we're /really/ just adding the `mut` to mean "scoped"... bah
  how do we define type aliases which can have effect arguments?
    e.g. we might want `Iterator(T)`, `const Iterator(T)`, `io Iterator(T)`, `atomic Iterator(T)`, etc.
    potential solution: _every_ type alias implicitly has an effect parameter
      which is used to instantiate _all_ free effect variables in the type definition (RHS)
      so you _can_ write e.g. `io Int` it's just not useful
      if you don't explicitly write effects it's implicitly polymorphic
        does this not cause problems if _everything_ has an effect variable?
        in a function def, _every_ type in it is instantiated with the same effect var?
          would we not get confusing error messages where one produces `mut Int`, and the other `const Int`?
      if you _do_ explicitly write an effect, does that mean "at least this effect" (still polymorphic), or "no other effects" (mono)?
        look at Frank?
      if type aliases are really just functions returning types... is this really the same thing as an effect parameter on _that function_?
        in this case...
          basic types like `Int` don't have an effect parameter (good!)
          all polymorphic types have an effect parameter (is this good?)
          monomorphic types have effect parameters only if they are defined as functions (good I guess?)
            let MyInt = Int; // no effect param
            let MyInt() = Int; // yes effect param
              (or `fn MyInt() -> type { type Int }`, whatever)
        the difference here vs. normal function effect polymorphism is that these functions are called _in the types_, at the type level
        does this make sense / work out correctly??
          the two clearly don't feel like the same thing...
  this solves `mut Array(T)` - can we do anything for growable vectors?
    am ersten blick, `Vec(T)` would always be mutable, could /not/ give out internal `Ref`s, only by-value indexing, and could have functions to turn into `mut Array(T)` and `const Array(T)`
    presumably this would need a double indirection, like `mut Ref(mut Array(T))`?
  when can copies in freeze/thaw be elided? (cf phasechange)
    when `return(freeze(foo))` is the final thing in a function returning an immutable array (or whatever)
    for `updateWith` (thaw-mutate-freeze), one of the two copies can be elided
      for chained `updateWith`s, the whole chain needs only a single copy
      (how can we elide even that single copy?)
    in general, whenever the value is used linearly
      monads can be used to ensure linear use of a single value (e.g. IO and State#)
      but that's only as long as it's type doesn't change(?)
      and if it does, we presumably need indexed monads again
      (though in this case, the type is _almost_ the same, because we have `mut Array` and `const Array` rather than `MutArray` and `ConstArray` - but that probably doesn't help)
w.r.t. scoped mutation, the usual way (e.g. ST monad) is that it (a function, expression) can be inferred to be pure if (a) it is polymorphic in the 'scope' / (b) nothing referring to the scope 'escapes'. but do we actually need this - given a function `Int -> Int` or whatever other pure signature, it seems inherently _impossible_ for mutation to escape it if it's not present in the signature. so we just let it do whatever it wants wrt its own mutable stuff because it has no way to leak them?
  or maybe the interesting thing is the closure - it could see a `mut Int` from an outer scope and the important thing is whether it touches that (in which case it's no longer `const fn` but `mut fn`)?
  what if it's not at the function but expression level, and we don't necessarily feel like typing out a (pure) type signature?
    syntax-wise, perhaps we could have `mut { ... }` blocks just like `atomic { }` and so on, which corresponds to `runST`?

an "effect" is basically anything where, given a HOF g(f) with parameter f, then g may do E iff f does E:
  nontermination (divergence)
  non-local jumps: exceptions, setjmp/longjmp... (coroutines? callCC? nondeterminism?)
  externally observable mutation, observation-of mutation
  I/O
  so that plain map() works like mapM in Haskell
  also invariant/contract violations! (`dangerous` and others)
    question is how to declare/scope such "effects"
    this would provide a clean story for privacy-peaking/internals-exposing as with eg Haskell `.Internal` modules
    accessing these would have the potential effect of violating the invariants of that module
      ("accessing"? heretofore effects were only attached to function calls...)
      (so instead of an Internals module there'd just be various functions with the dangerous(this-module) effect?)
      or maybe we actually want record fields to have effect types, so we get something like the proposed `unsafe` struct fields for Rust...???
    invariant violations, a.k.a. abstraction violations
      the language / type system _itself_ is an abstraction (violated by `unsafe`/`dangerous`)
http://lambda-the-ultimate.org/node/4786#comment-76148 dmbarbour on effect systems

FFI integration
  just amounts to specifying/calculating the size, alignment, and GC routine (as function of the type parameters)?
  type Type = (sizeof: UInt, alignof: UInt, trace: (dangerous?) fn(somethingsomething) -> somethingsomething)
    this probably bottoms out at mutual recursion somewhere...
    like we _also_ want `type Type = (sizeof 12, alignof 4, trace ...)` or whatever
    so this is like a `Type: Type` thing
    in actuality given the first def, the second one actually follows from `type Record([(Name, Type)]) = ...`
    like where do we actually end up having to tie a knot - Type? Int? Record?
    _actually_, these are two separate things...
      (a) on the type-level, for the typechecker, the actual type
      (b) somewhere below that(?), for the code-generator, the runtime representation
      types with the same runtime repr are not automatically the same type!!! of course
        although, structurally... it makes some sense for all `CStruct([CInt, CInt, CFloat])` to be the same type
          after all they are compatible
          just like all `(a: Int, b: Int, c: Fract)` are the same type in Ducks
        and if you want to make things abstract, you can do so as a separate matter in both cases
        this suggests that at the most basic level, all that a type is _is_ its repr, and these are structurally equal
          and things like the type-constructors for records, CStructs, enumerations, etc. are themselves somehow abstract types layered _on top of that_
          on another level, type-as-size-alignment-etc is _inherently_ abstract, there's nothing at all you can do with it...
            (plus you _shouldn't_ normally be able to inspect these, because parametricity etc)
      so what we want to do in the FFI case is something like declare an abstract type along with its repr
        where do the two separate...? if we view our own records as just type constructor functions themselves, and so on...
  type CPoint = (sizeof 8, alignof 4, trace fn() { })
  type CPair(A, B) = (sizeof A.sizeof + B.sizeof, alignof max(A.alignof, B.alignof), trace fn(ptr) { A.trace(ptr); B.trace(ptr + A.sizeof) })
  this basically presupposes having some kind of compile-time constants though...
    like we'd actually want `fn CStruct(fields: [Type]) -> Type`
  this kind of thing would seem to be in tension with relative pointers and SHM and so on...
    of course we can have separate types for relative and absolute pointers
    perhaps "absolute-address pointer" could/would/should coincide with "pinned in memory"?
    which is fine as far as it goes, but how do we track which composite/abstract types it's safe to share/send/relocate/whatever?
      (but maybe we need this kind of thing anyways?)
  WeakRef(T) could basically exist on the same model as Ref(T), maybe with actually the function `fn upgrade(WeakRef(T)) -> Ref(T)?`
  what about finalizers (and ducks-to-foreign references)?
  what about stable pointers (foreign-to-ducks)?
    same thing as pinning?
    perhaps along with rooting?
    maybe Root(T) would contain a relative pointer _which gets updated by the GC_
    in which case it's separate from pinning
      which is what you would need if you want to pass an actual C pointer around
      when/why would you want that, though?
    in either case how is the lifetime managed? just pin/unpin root/unroot, reference counting, ...?
      perhaps it makes sense to also have scoped HOF (withPinned etc) versions of these
      (with cleanup/unpin/unroot/w/e unconditional on scope exit, or some kind of ST monad trick?)
  `Ref(T)` seems similar to `StableName` -- immutable data but with identity / address equality?
     in which case we should be careful not to break purity with it...?
       (maybe we should only allow _creating_ them from mut/io, but testing their equality from pure code afterwards is OK?)

if we want to be nesting-level agnostic (no special treatment for top-level or local fns) then where does `return` return to?

The status of Int
  We want to be able to pattern match on it
  Everything you can match on the cases of should actually be an enumeration
  So Int should be an enumeration (case 0, case 1, case 2, ...)
  These could actually be exactly "anonymous (duplicate) cases"! (dual to tuples)
  type OneTwoThree = (case 1, case 2, case 3) - "subtype" of Int
  type Maybe(T) = (case 1(T), case 0)
  but _actually_ this is all about `UInt`!
  You can't have duplicate-labels at negative indices
  So what actually is the status of *Int*? Add a sign bit? Something else?
    What's the nicest way to do this that avoids having +0 and -0?
      maybe something like `type Int = (case ZeroAndUp: UInt, case MinusOneAndDown: UInt)`
  This is also connected to whether indexing (incl. presumably field/variant duplicates) starts at 0 or 1
    Probably this should determine that?


Overloading / ad-hoc polymorphism
  To what extent / in what ways are these the same / different things?
  W.r.t. MTC / Modular implicits, the part we probably we're probably interested in is writing _generic_ functions
    E.g. when you have
      type Num(T) = (add: fn(T, T) -> T, mul: fn(T, T) -> T, zero: T, one: T)
      fn sum(type T, NumT: Num(T), nums: [T]) -> T { fold(elems: nums, init: zero, op: NumT.add) }
      fn dotp(type T, NumT: Num(T), left: [T], right: [T]) { sum(nums: zipWith(op: NumT.mul, left, right))
    here because `T` is abstract, the only possible thing of type `Num(T)` when calling `sum` is `NumT`
      what about the names? do they need to match? should they be arbitrary? should they be anonymous and implicitly glob-imported (like TCs)?
    and perhaps the same thing in the dual case of a module exporting an abstract type
    this can't really work in the "top-level"/specific/monomorphic case because types are structural so there's no concept of "the impl for Point = (x: Int, y: Int)"
      and this would also somehow require us to solve the problem of multiple things with the same name but different types existing in a given namespace...?
      perhaps this is what the "overload .. from .." construct from MTC/MI is meant to address
    so this would only work for polymorphic code calling other polymorphic code, not monomorphic code calling into polymorphic code
      does this in any sense happen to correspond to the stratification MTC (but not MI) imposes?
        so instances can only be declared at the outer level, while type inference / implicit polymorphism only happens at the inner level
        and this somehow ends up preserving coherence
    potential complication: `Num(T)` is _itself_ structural? so the name of the type is immaterial, and anything with the same structure should "fit" / "be found"?
      (but modules in MLs are also structural, so maybe this isn't an issue...?)
  OK, so what about the monomorphic case then?
    if I have separate types Int, UInt, Rational, Real, ..., how does it know which Add to pick?
    OTOH, this is basically the dual "modules exposing abstract types" case from above
    what's a better example?
    besides type-based overloading, we may also want parameter-name-based overloading a la Obj-C
      but only in the "first-order" case
  (how does effect typing fit in here? like if you declare a Monad interface are / should the contained functions then also be effect-polymorphic...?)

Gradual typing: if we wanted it, what might it look like?
  and how would we ensure abstraction/parametricity isn't violated?
  starting point is that it's obviously an existential type - `(type T, foo: T)`, with "something extra"
    as long as you can't take out more than you put in, parametricity should be OK?
  some kind of TypeID, or an actual cast function...?
  remember we actually have structural types here...
    does dynamic typing even make sense in that context?
  probably instead of e.g. GHC's nominal-types-and-type-ids-and-type-casts-based Typeable/Dynamic system
    we want something more like Jeremy Siek's system with those "dynamic eta rules"
    ie given `x: Dynamic`, we don't want to ask "is x of type Foo?"
    we want to ask "does x have field y?"
      and perhaps "can x be called with an Int argument?"
      (though this _is_ similar to "is x of type Foo?", only Foo is matched structurally...?)
    what about "does x support a method bar"?
  there was pigworker's insight that `*` ("Dynamic") is the fixpoint of all of the defined datatype-functors...?

is a thunking pattern match (`let foo() = a + b`, or `let foo! = a + b` in Frank) kind-of the same thing as irrefutable patterns `~foo` in Haskell?

"Then, as in any dual construction, the introduction form of the primal corresponds to the elimination form of the dual. Thus, elimination forms of sums (e.g., match) correspond to introduction forms of records. In particular, record extension (an introduction form) corresponds to the extension of cases (an elimination form). This duality motivates making cases first-class values as opposed to a mere syntactic form."

Duplicate effects in effect rows
  According to Koka this makes some things easily typeable like an expression catcher where the handler itself may throw another exception
  Does it have any bearing on nested atomic transactions?

stuff like HList and flexible sum types require immense type-system effort in Haskell&co. and everyone ends up wanting them... here they are basic
    the other thing with HList, though, is that it has an order and you can iterate through it
    for this we can use many fields with the _same_ name, which do imply ordering?
      if we also have anonymous fields, we recover variadic tuples...
      is it easy (or possible) to do induction over these?
        can we just translate the Haskell implementation into an explicit dictionary-passing one, and that to Ducks?
also vertical lambda composition!
  (is this the same thing as a "pattern calculus"?)
  can this _completely_ replace a `match` construct?
    including all the fancy features like nested patterns, conjunctive/disjunctive patterns, ...?
    given that functions are _inherently_ defined over records, this automatically(?) gives us the matching-multiple-patterns-at-once thing
    is a disjunctive pattern just if a single function (arm) handles multiple cases of an enumeration?
    does this give us the usual top-down matching, or do the types require it to be something more restrictive?
    what about pattern guards, and esp. fall-through to the next pattern if it fails?
      (maybe this is where pattern-match-failure exceptions would be useful...?)
    two ways to formulate this(?)
      to get result `fn((case foo(A), case bar(B))) -> Z`:
        * `fn(case foo(A)) -> Z` plus `fn(case bar(B)) -> Z`
        * sum `(foo: fn(A) -> Z, bar: fn(B) -> Z)` (first class labels)
  "Switch statement that doesn't suck? It's called hash table storing functions."
  can we also get something like pattern synonyms out of this? (not just "arm synonyms")
    maybe with some kind of compile-time constants / constant functions (a la Rust)?
  can `if`, `if let`, incl. as pattern guards, `match`, vertical lambda composition be put into some kind of unified framework...?
    `if` boils down to `if let true = ...` ... what about the boolean operators?
  https://www.reddit.com/r/programming/comments/lirke/simple_made_easy_by_rich_hickey_video/c2uqctu (julesjacobs) patterns as invertible functions
can algebraic structural sums/products also help us with improving relational algebra (first-class types, NULL -> sum types)?

First-class record/variant/label stuff
  parameterized over rows: records, enumerations, relations... other? SoA/AoS?
  map() on rows?
    like `instance Functor (Map k)`: only acts on types, leaves names/labels alone
    e.g. To(R: Row, Z: Type) = R.Foreach(fn(T) { type fn(T) -> Z })
    given an enumeration (A...), Record(To(A...)) is the type of records-of-functions to pattern match on the enumeration
  allow splicing one record into another as long as at most (or exactly?) one of them has a row variable?
    a la effect rows in Frank

meaning of `const fn`
  constant over the lifetime of the program, probably?
    people will be using it this way anyways, and otherwise friction
    pragmatism(tm)
    ...well, except if we want to do things like send closures across the network!!
  do "output-only" effects which can't be observed from pure code but _can_ from IO still count as pure??
    i.o.w. "side effects"... this actually seems somehow dual to "referential transparency"
    in some sense this is unavoidable: memory usage, time passing during eval, etc. are all observable side effects!
    maybe this kind of pure-functions-can-throw-and-only-IO-can-catch should be our answer to Rust's `panic!()`?
      i.e. only in case of unrecoverable/programmer errors, but with harm minimization (don't abort whole server process e.g.)
      (actually, GHC and Rust do seem pretty similar in this respect...)

Nonlocal control flow and concurrency things
  return break continue goto
  Exceptions
  Yield / generators
  Delimited continuations
  Undelimited continuations
  setjmp/longjmp
  Coroutines
  Async/Await
  Future, IVar, FRP Event
  Threads
  Actor model
https://www.reddit.com/r/programming/comments/3i1l9n/from_imperative_to_purefunctional_and_back_again/cue8tgv?context=3 julesjacobs
https://www.reddit.com/r/haskell/comments/3hj9hb/we_should_be_able_to_use_monadic_predicates_when/cu8y1mz

delimited continuations ~ yield/run ~ algebraic effects??
Yield ~ IEnumerator ~ unfoldr?

relationship between first-class unification and logic programming...?
  invertible functions, lenses/prisms/traversals, coroutines, ...?
  delimited continuations, yield/run, algebraic effects...?
  http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/
    http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/#comment-1830835050
  http://blog.paralleluniverse.co/2015/08/07/scoped-continuations/
  "Does it provide channel select? Because that's the hard part. Threadsafe queues are easy, select on them is hard."
  "Thread memory use is kind of independent of 1:1 vs. M:N, honestly; the thing that can improve scalability is relocatable stacks, which is really not the same thing."
    stacks are just (a linked list / cactus stack of) records, so...?
  "any sort of userspace threading won't get you to the performance of, say, nginx, as once you have any sort of stack per "thread" you've already lost."
  "So since you're talking about scalability into the millions of threads, I think what you actually want is stackless coroutines rather than M:N threading with separate user-level stacks. If you have 1M threads, even one page of stack for each will result in 4G of memory use. That's assuming no fragmentation or delayed reclamation from GC. Stacks, even when relocating, are too heavyweight for that kind of extreme concurrent load. With a stackless coroutine model, it's easier to reason about how much memory you're using per request; with a stack model, it's extremely dynamic, and compilers will readily sacrifice stack space for optimization behind your back (consider e.g. LICM).
  Stackless coroutines are great--you can get to nginx levels of performance with them--but they aren't M:N threading as seen in Golang. Once you have a stack, as Erlang and Go do, you've already paid a large portion of the cost of 1:1 threading."
  "Coroutines are preemptible at I/O boundaries or manual synchronization points. Those synchronization points could be inserted by the compiler, but if you do that you're back into goroutine land, which typically isn't better than 1:1. In particular, it seems quite difficult to achieve scalability to millions of threads with "true" preemption, which requires either stacks or aggressive CPS transformation."
  "Coroutines differ from green threads in that you don't yield to the scheduler, but you explicitly yield to the coroutine that you want to run. In this example, the lexer would explicitly yield a token to the parser, and the parser would explicitly yield to the lexer when it needs a new token. This is fully deterministic at all conceptual levels. You can implement green threads on top of coroutines by writing a yield-to-scheduler function that picks another coroutine from a list of suspended coroutines, and yields to it." --julesjacobs
  (do FRP Behaviors fit in anywhere?)
    if FRP.Event = Future, and green threads / coroutines / fibers are preferable to async and Futures...
    ...where does that leave FRP? can we write Event-based FRP code as imperative code?
    are green threads / coroutines / fibers just the same thing as the Event monad?
    what about EventStream and Behavior though? when used in conjunction with Events?
http://www.paolocapriotti.com/blog/2012/06/04/continuation-based-relative-time-frp/

what should the basic "stream"/"iterator" type be?
  persistent lazy lists, like Haskell?
  mutable in-place imperative iterators, like Rust?
  the Stream stuff behind stream fusion is essentially... (one of these?) a fold? unfold?
    I think it was an unfold...
    except specialized to lists-with-filtering, whereas it could be datatype-generic in theory?
  internal or external iterators or both??
    first-class continuationy (non-local return) stuff makes internal iterators viable
    but it can still be nice to pass around Stream(T)/Iterator(T)/List(T) explicitly?
    what does Haskell do?
      it has both: internal iteration with fmap/mapM, external with [a] and stream fusion?
    is there still a place for external iteration in a world with first-class delimited continuation ish things?
      yeah - you can't do things like InterleavedIterator with internal iterators(?)
      actually you can... non-local return isn't enough, but fibers are!

"Sure there is: nonlocal control flow is any control flow that is initiated outside of the current function, and thus, at best, requires global analysis to reason about. At worst, when combined with indirect calls, it is impossible to reason about in all cases." -- whitequark

what's the deal with generalizing booleany stuff to patterny stuff?
  an irrefutable pattern match essentially returns a record of bindings
    what is its type?
    (refutable just means "on a record, rather than enumeration type")
  a refutable match returns either a record of bindings or an enumeration of the remaining variants
    (or rather, the "record" of bindings is only if it's an irrefutable match inside a refutable one)
    what is its type?
    how can it avoid an infinite regress (having to refutably match again on the result)? CPS HOF?
  what about: conjunctive patterns (@/as), disjunctive patterns, horizontal patterns (just records?), nested patterns, guards...
     what about `let P(x) = foo() else bar() else baz() else return();`, `let P(x) = f() else let Q(x) = g();`?
     what primitives can we express these with? can they be made ergonomic to use _directly_?
  what about !, &&, ||, ...?
    what do they apply to - enumerations, patterns, pattern matches...?
  if->if let, while->while let, unless let, until let, vs. loop-break ...
    while foo { stuff } -> loop { if !foo { break } stuff }
    while let foo { stuff } -> loop { if !let foo { break }; stuff }
      what's that !let there??
      probably something about the pattern match having two continuations - success and failure - and they get flipped...
  `case foo VAL` is dual to `VAL.foo`
    prefix injection vs. postfix projection
    what about the other half - record construction vs. pattern matching?
    there's also "with .foo = 5" and "set .foo = 5" -- record extension/update (builder pattern)
      what are dual to these?
    maybe put this in the syntax?
      `(foo: Int, bar: Bool)` -> `(.foo: Int, .bar: Bool)`
      `(foo 1, bar true)`     -> `(.foo 1, .bar true)`
        but we don't want to write it like this for functions :-(
      `(case foo: Int, case bar: Bool)`
      `my_rec.foo` -> 1, `case foo 1`
      what about pattern matching??

threading model, concurrency & parallelism
  if we memoize pure fns and also have threads then we need blackholing and stuff? :\
  we definitely want `mut` stuff to be single-thread only
    how do we prevent it from leaking across threads?
    how does Haskell prevent it?
      because you can't call IO functions within the ST monad... ?
      would that mean we can't do IO and local mutation at the same time?? not so nice...
      iow, `io` and `mut` effects would be mutually exclusive
    alternatively, we'd need something like Rust's Send/Sync/'static??
    or maybe "multi-threaded" should be a separate effect from `io`?
  do we want to allow shared memory between threads?
    if not: garbage collection can be simpler, never need to stop all threads at the same time
    this means that
      when sending something to another thread, it needs to be copied (~ Erlang)
        need to copy the whole transitive closure, ~ GC tracing?
      cross-thread primitives need to be atomically reference counted
        each IORef essentially gets its own heap?
          it has to be a pointer-to-a-pointer because we need atomic CAS and types are _not_ all pointer-sized
          when writing to it, copy all the data into a single memory block, and then atomically swap it with the old block, and then free the old block?
          what about when reading?? how do you copy it out without another thread swapping it out and freeing it in the middle?
            do we actually need a mutex?
        can the Rust-like Mutex thing even work here?
          so you get an exclusive access via a scoped `mut Ref(T)` to the contents for the duration of the lock
          (how) would the thread-local GC know not to free the memory that `Ref` points to...?
        also, of course, this means that cycles can leak... how bad is that?
          (or are cycles even possible? under what circumstances?)
      pure functions should still be okay to evaluate in parallel "within a thread"...?
        (three-layer cake?)
        so we still need blackholing on `const fn()`s if we want memoization
          "As another example, it’s well-known that on x86, a 32-bit mov instruction is atomic if the memory operand is naturally aligned, but non-atomic otherwise. In other words, atomicity is only guaranteed when the 32-bit integer is located at an address which is an exact multiple of 4. Mintomic comes with another test case, test_load_store_32_fail, which verifies this guarantee. As it’s written, this test always succeeds on x86, but if you modify the test to force sharedInt to certain unaligned addresses, it will fail. On my Core 2 Quad Q6600, the test fails when sharedInt crosses a cache line boundary:"
        how badly does this complicate the GC?
      what does this mean for `atomic { }`??
      what about for concurrent revisions?
    is this plus relative pointers all that we need to use processes instead of threads?
    can we get similar GC simplifications in a shared-memory context by avoiding some other particular things?
    if we have this kind of task memory-isolation then do we also want a type for single-threaded-but-not-scoped mutation?
      i.e. non-atomic, but `io` rather than `mut`
      (and then we can also allow writing into `Ref`/`Array`s with this effect along with `mut`?)
      (in effect it would _actually_ correspond to `static mut`, instead of "`static mut` plus threading"?)
      (does this even make sense considering our priors??)
      right -- this is essentially the same thing as `ST RealWorld` in Haskell, or `'static mut` in Rust
      so it's logical, and we presumably want it (maybe `global mut` or `mut(global)` or something, if we need syntax)
      question is whether _all other_ IO should also go here, or just globally scoped ST stuff
      (and whether there's any kind of meaningful distinction between "concurrent" IO/mutation and non...)
    if we have memory isolation we'd maybe also like failure isolation a la Erlang?
      how do non-memory resources figure into this?
      "Here's the real-world facts that need to be dealt with:
        * All code can fail, because all code can have bugs.
        * Obviously, we don't want every single function everywhere to return Result<T, E> as a way to signal arbitrary "I had a bug" failures. This is what panic is for.
        * In a server handling multiple concurrent requests from different users, it is not OK to kill the whole server when some code fails. Servers need to be fault-tolerant. Only the request in which the error occurred should fail. If you want to "fail loudly", arrange monitoring such that someone gets paged when such a failure happens; don't return errors to 100 unrelated live users.
        * In an event-driven server, failing a thread is approximately as bad as failing the whole process in this regard, as each thread handles many concurrent requests.
      "
      probably not with unwinding like Rust... probably we want something like per-task oops handlers?
        which then get to run finalizers?
        how is this actually _different_ from unwinding on a semantic level?
  even if we do, we do still definitely want data-race freedom
    i.o.w. we want soundness and undefined-behavior-freedom :o)
    so at least IORefs need to be pointer-to-pointers for atomics/CAS (because sizeof(T) is not always sizeof(Word))
      (is this an implementation detail or should the interface actually expose that all operations are performed on data of type Ref(T)?)
        e.g. fn new(type T, ref: Ref(T)) -> Atomic(Ref(T))
        has the advantage that we can directly overload it for other types which are guaranteed to be word-sized?
          e.g. fn new_int(int: Int) -> Atomic(Int)
        all other operations can be generic, only the constructors need to be smart
        perhaps we can also have:
          fn new(type T, val: T) -> Maybe(Atomic(T)) { if T.sizeof() == Word.sizeof() { case yes = ... } else { case no } }
        this is kinda nice but not sure if it passes complexity cost-benefit test over just making it completely generic over T and using Ref internally
          (like, should it be called Atomic or just IORef)
  https://hackage.haskell.org/package/stm-containers
    we should be able to do things like this?
    can we do it while being generic over pure, ST, IO, and STM at the same time??
    does it _make sense_ to be generic over STM and ST/IO? do the same algorithms "just work"?
Look at Pony?


Precise semantics of uppercase vs. lowercase, type vs. nontype, compile-time vs. runtime, abstract vs. concrete, generative vs. applicative, (im)predicativity and (non)dependency, ...
  VERDICT: (BLOCKING) This is important!
  maybe what we want to track is "known to the typechecker" or not?
    so abstract/concrete and runtime/compiletime coincide?
      which "things" can you have at the type level?
        definitely types, records, functions
        cases, branches, ints?
      we also want some degree of coincidence with inferred/not? so that explicit type application is just foo(T MyType, ...)
    where "known" means "can see the definition"
      are compiletime/runtime and applicative/generative (pure/impure) orthogonal?
        runtime_pure(T) == runtime_pure(T)
        runtime_impure(T) != runtime_impure(T)
        Compiletime_pure(T) == Compiletime_pure(T)
        Does Compiletime_impure even make sense??
        how do we know whether one `T` == the other `T`??? especially if it's, like, a function...
          blahrg equality/extensionality :-(
    if we have a pure lowercase function returning a type, the typechecker can always know that the output type will be the same, given the same input type
      so we know that f(T) = f(T)
      given `let U = f(T)`, do we also know `u = f(T)`? I mean, of course we *should*... (why does it feel weird?)
        maybe because... it feels weird to "remember" what function call a particular value came from?
        (except in this case, it's at some level all at the type level...)
    if we have a pure uppercase function it can also _see the definition_ and discover that eg F(T) = G(U)
  do we actually have some form of subtyping here, with uppercase <: lowercase?
    how does this interact with contravariance and relate to the usual paradox/undecidability thing that tends to occur with that combination?
    yeah, if the issue ML has with undecidable signature matching is going to occur for us, it's going to occur here
  also the parametricity implications... can `fn(type) -> type` inspect its argument?
    and if we have type-level constant data...
    (I guess that's only if we gain some form of type-level sum types -- otherwise, it just boils down to what we allow for `type`)
    this also feels like some sort of abstraction... like maybe plain `type` is opaque, but if there's e.g. Int actually visible in the signature, that can be branched on?
    this probably also connects to what the introduction/elimination forms of `type` are...
      this seems really interesting!!
      what _are_ they?
      on one level, intro forms are record + enumeration + function + ...types
        (what are the corresponding elim forms?)
        but are these really _primitive_?
        and where does `foo: T` fit in? is that some kind of weird-ass elim form...?
      is `type` even a positive or a negative type?
      intro and elim forms are "how you make a thing" vs "how you can use it"
      intro forms of negative types consist of specifying what happens when you use it
      elim forms of positive types consist of inspecting how it was made
      according to the HoTT book, universes or at least their identity types are negative
    we don't _want_ people case-switching on it. it's not a nominal type. nominal types are something like structural type + unique ID (both type-level and value-level: AutoDeriveTypeable). here we have structural types. if we want some kind of global-unique ID either we generate it with a generative functor (maybe), or we use type-level structural enumerations (maybe), or something.
    idea: we say that structural sums, products, functions, etc. are the available types, but we never say these are the _only possible types_. so pattern matching on cases here is philosophically unsound (or something)
  dependency means:
    fn foo(b: Bool) -> type {
        match b {
            case yes -> Int
            case no  -> String
        }
    }
    fn bar(b: Bool, x: foo(b)) -> Int {
        match b {
            case yes -> x
            case no  -> length(x)
        }
    }
    iow: ???
    iirc "dependency is whenever you can gain information about types by inspecting values" (or something)
  impredicativity means:
    Array(fn(type T) -> Array(T))
    iow instantiating something of type `type` with something that is itself, or "mentions"/"includes", `type`
      we obviously want this, I think.
    actually, impredicativity means quantification over `Type` is still in `Type` (ye olde forall/exists)
    but `Type` itself, and things that e.g. _produce_ types, are not!
      what's the precise distinction? does it have to do with positive/negative positions...?
        not quite, because existentials are positive
          (maybe that gets to be lowercase -- "in Type" -- because/if the other component of the tuple is lowercase?)
  can we have types parameterized over modules?
    well, of course we can, in the sense that you can physically write it...
    when are two such types considered equal? given `type Map(T: Type, O: Ordering(T))`, how do we make sure different orderings are inequal?
      (generative modules, or is there something prior/better?)
      values are always considered inequal unless they are actually the same value (as in "same variable")? (or a pure function of same values, etc)
  one question wrt uppercase things is... what _can't_ you do with them?
    what prevents you from writing the entire program uppercase, or why wouldn't you want to?
    maybe we won't have any kind of branching at the type level, at least initially?
  is the type level itself dependent??
    uppercase things can't depend on lowercase things, OK
    but can uppercase things indiscriminately depend on uppercase things?
    can you write `(Foo: Type, Bar: Foo)` or w/e??
      of course this example arises if we have type-level constants... does the general question arise otherwise?

How does every-thing-is-a-record-containing-functions-and-types fit in with row polymorphism and specialization and so on??
  For enumerations, we'd want RP when it's in the return type??
  Does row polymorphism give us signature matching??

UPPER VS. LOWERCASE, MODULES, ...
  What property are we trying to uphold and why????
  --
  foo: (type T, ...)
  let X = foo.T; // can you do this?
  let x = foo.T; // what about this?
  --
  fn f(b: Bool) -> Type { if b { Int } else { Float } }
  fn F(b: Bool) -> Type { if b { Int } else { Float } }
  --
  B: Bool
  fn f(B: Bool) -> Type { if B { Int } else { Float } }
  fn F(B: Bool) -> Type { if B { Int } else { Float } }
  --
  T: Type
  X: T
  (how come impredicativity and type-level constants seem to be entangled...?)
  --
  We don't want singleton `(type T = Int, combine: fn(T, T) -> T)` types like ML, only abstract `(type T, combine: fn(T, T) -> T)`
    Factored out: `type Monoid = (type T, combine: fn(T, T) -> T)` and `Monoid where type T = Int`
  Instead of ML's translucent types and signature matching (which requires subtyping), we want to write:
    `type Monoid(T) = (combine: fn(T, T) -> T)` and `Monoid(Int)`
    Where does this leave us at a disadvantage?
  If we want to eschew subtyping entirely, that means we also need a different solution for being able to apply different signatures to the same module
    Can this actually work with row polymorphism?
  What happens if we make Names, Rows, and/or Effects first-class things which can be elements of records, just like "normal" types?
  Distinction between data abstraction and type abstraction?
    Data abstraction is a record where not all of the fields are exposed (private definitions)
      This is private definitions in _modules_; private fields of _structs_ correspond to abstract types
      Or does the below give us both? Seems like it might...
      (There's not much algebraic/practical difference between an abstract type and a record type entirely consisting of an abstract row (struct with all fields private), but they are nonetheless different things?)
    Corresponds to an existentially-quantified row?
      let MyMod: (a: Int, ...) = (a Int, b Float, c Char);
      (maybe some kind of `private` keyword that makes it _infer_ that way? or a different kind of record literal which inverts it and requires `public` to expose?)
    And then I guess this would extend trivially to enumerations with existentially quantified rows as well ("private variants")... nice
  What we essentially want: lowercase stuff is out of scope for uppercase stuff?
    (And uppercase stuff is out of scope for lowercase, except on right of `:`?? cf pigworker System F oil/water)
      no, that would mean we can't have modules (uppercase ones anyways) and access their lowercase components in terms...?
      but if yes, does that mean we would have type-level constants...?
        well, uppercase modules essentially _are_ type-level constants?
        so the question is then only compile-time constants _of what types_...
  How do we write a module exporting an abstract type? What about a module exporting a type synonym? Which is the easy one??
    The default clearly has to be abstract(?), consider e.g. parametric polymorphic functions - they receive a type parameter as argument, and it is abstract
    And in systems with translucent modules, `type T` (as opposed to `type T = Foo`) also means an abstract type
    Questions:
      * In systems with translucent modules, what do you actually _need_ `type T = Foo` for?
        Instead of `(type T = Foo, foo: T)`, you can always write `(foo: Foo)`
        To "forget" you can just extend the record with a `type T`, and existentially abstract it: `(type T, foo: T)`
          This is nice, because it no longer involves subtyping
        Sharing constraints... are handled by this kind of thing and/or parametrization-over-fibration?
      * How do we export a type synonym? Why would we want to?
        Relevant comparison: Rust modules and type aliases vs. abstract types, also consts and const fns?
          `mod foo { const N: i32 = 5;         }` does the typechecker "know" that `N` is 5? (that's the whole point??)
          `mod foo { const fn N() -> i32 { 5 } }` what about now? same deal?
          `mod foo { type Foo = i32;           }` here everyone knows that `Foo = i32`
          `mod foo { abstract type Foo = i32;  }` here only `foo` knows that `Foo = i32`
        In each of the cases what is the "type" of `foo`? Does it matter?
        How would we express each of the last two in Ducks?
  What would `reflection` look like in this setting?
    reify   :: forall a z. a -> (forall s. Reifies s a => Proxy s -> z) -> z
    reflect :: forall s a. Reifies s a => proxy s -> a
    fn reify(T: type, R: type, x: T, f: fn(X: T) -> R) -> R ??
      IF this is OK... it essentially means we can instantiate uppercase parameters of lowercase functions with lowercase arguments???
      How is this not dependent types??
        Maybe because R should not contain uppercase stuff... or something?
    what is the Haskell version actually _for_? making it be parametric? implicit passing?
  Impredicativity + case-analysis (non-parametricity) is bad
    We don't want typecase anyways, but what about for other uppercase things?
    Can you compile-time branch on `B: Bool`?
  In "ML Modules and Haskell Type Classes: A Constructive Comparison", associated types in `impl`s can be either `type` or `abstype`
    what does that correspond to in Ducks?
    what does "type abstraction in Haskell is performed inside instance (and not class) declarations" mean?
  Possibilities:
    * Lowercase stuff is out of scope for uppercase stuff
    * Only uppercase stuff may appear to the right of `:`
    * Uppercase stuff is "transparent to the typechecker" or "compile-time constant"
    * Uppercase stuff is parametric
  Boundary conditions: compositionality and modularity!
  A fundamental issue is that (as currently formulated) function return types do _not_ have capitalization
    So we _can't_ say "uppercase outputs may only depend on uppercase inputs", because we don't know when the output is uppercase!
    (And we can't infer it from use sites without destroying modularity)
    What can we do:
      * `Type` is inherently uppercase and other types aren't (what about functions and records, and generic return types??)
      * We add a way to signify the capitalization, and/or functions may only return records (which then have upper- and lowercase components)
      * The capitalization of the return type is signified by the capitalization of the _function_
          but what about lambdas?
          is this logically consistent with the universal meaning of uppercase, as applied to the function itself as a value??
            "runtime function which is known at compile-time" (`const FOO: fn()`) vs. "compile-time function" (`const fn FOO()`)
            are these different things??? how is this different for functions than like, Ints?
            (the value of an Int is a number... the value of a function is its body, isn't it? what does it mean to know a function at compile time but not its body??)
      * We figure out a way to make it not matter, and impose restrictions directly+only on uppercase arguments (values) and/or functions
  Taking the last one: what if we take "uppercase values must be handled parametrically" as the starting point?
    What _concrete_ restrictions does that imply?
      `fn(T: Type, x: T, y: T) -> T` is isomorphic to `Bool`
      so... can we not apply it??
    What further restrictions are necessary to achieve our aims?
    if `fn(A: Foo, a: Foo) -> Foo` can return either one, that definitely destroys any notion of "uppercase variables can't be instantiated by lowercase stuff"
      can anything still be saved? what do we want to save?
  "opaque" vs. "transparent"
  for meaning of uppercase things, are we conflating big/small (universes) with transparent/opaque?
  "normal value level things" are always "opaque" -> intuition that lowercase should be opaque, uppercase transparent?
    fn f(X) --> `X` should be opaque to `f`
    fn F(X) --> `X` should be transparent to `F`
    ?
    but what does transparency even imply in this context?
    above we said "uppercase should be parametric", here we say "uppercase should be transparent"... these are direct opposites!!
  opacity vs. transparency only matters at the type level, because it determines whether or not we can infer two types to be the same...
  there's no static checking of this sort for the value level, naturally
  and maybe reflecting values to the type level is OK as long as it's opaque?
  maybe: "lowercase things should be opaque to uppercase things"? (what about vice versa...??)

TYPE INFERENCE
  the classic MLF example `choose id`
    can be given two System F types
      forall a. (a -> a) -> (a -> a)
      (forall a. a -> a) -> (forall a. a -> a)
    and the MLF type
      forall (b >= forall a. a -> a). b -> b
    which can be instantiated to either of the previous.
  can we express that with Ducks types? (are Ducks types just System F-like, or more than that?)
    if >= is something like a subtyping relation, can we use a function instead...?
    System F types:
      fn(T: Type) -> fn(fn(T) -> T) -> fn(T) -> T
      (fn(T: Type) -> fn(T) -> T) -> (fn(T: Type) -> fn(T) -> T)
    MLF type:
      fn(B: ??? fn(T: Type) -> fn(T) -> T) -> fn(B) -> B
      B is a supertype of ...
      we need a function that can either instantiate the `T`, or leave it unchanged
  "higher-order (pattern) unification"?
  "Partial polymorphic type inference and higher-order unification"?
  "unification-based type inference does not have a direction and this places severe constraints on how your lang can look like"
  "for example, if your + operator casts operands to widest type of both, it will inevitably require annotations"
  "every time you introduce an implicit cast you prevent inference from flowing in either direction across the cast" -- whitequark

AD-HOC POLYMORPHISM, OVERLOADING
  important part: be able to declare abstract interfaces, overload based on them (not just "best matching signature" a la C++)
  one potentially key difference vs. MTC/MI is that we have Monoid(Int), not Monoid where T=Int
  "T type class is an addictive, unprincipled, global, piecewise-defined, implicitly-applied partial function from types to a record." - right
    can we maybe use type-level structural variants for this actually somehow...?
  Dreyer, Why Applicative Functors Matter:
    > Treat projection from a functor as a composition of projection and the functor: F.l := (.l) o F
    > “MkSet.insert” = λ(X : ORD). MkSet(X).insert
    > “MkSet.t” = λ(X : ORD). MkSet(X).t
    > For MkSet.t, argument cannot be inferred
    > Projection and application of a functor commute: F(M).l = F.l(M)
    But of course: this is just like `for<T> { ... }`, a.k.a. an item in a polymorphic module ~ a polymorphic item
  http://www.haskellforall.com/2015/10/explicit-is-better-than-implicit.html

DATA-GENERIC STUFF
  what's our answer to `deriving`?
  enable writing implementations generically over records/enumerations?
    so it's "automatically available" for structural algebraic types based on their components
    abstract types can just re-expose it
  where does this approach fall short - what's the kind of thing where GHC.Generics isn't good enough and you really need Template Haskell? (data-generic vs. macros)
  to make this really work we need to enable explicit(?) polymorphism over _everything_ - names, effects, rows, ...?

everything-is-a-record vs. mutability of individual variables / arguments
  fn foo(mut x: Int) and fn foo(x: mut Ref(Int)) are manifestly distinct
  how does `let mut foo = 5` translate into a record field?
  do we even have `let`s? or just `name value`?

everything-is-a-record vs. the stack
  interesting case: closures
  here the closure (fn) sees _three_ "records":
    its arguments, its local variables, and the outer environment
  how are these combined/coalesced/concatenated/cowhatevered?

"We must pass offsets at runtime for constant time field access. Of course, the CPU already carries this offset for us: it's the stack register, and the environment is the stack. Record extension is thus pushing a stack frame, and record selection is accessing a stack variable."


is the kind of functions `(Effect, Type) -> Type` or `(Effect, Row) -> Type`?
  latter seems more appealing?


how to handle the tension between FFI and concurrency?
  for one thing, stacks
  and sending closures between processes
    if it closes over FFI pointers, that won't work
    but tracking which closures (which existential types) are sendable at the type level is painful at a minimum
    compromise: allow runtime test for sendability on any type / closure?
      "worse is better" :/
    or maybe: `extern fn` as the type of not just foreign functions, but any functions which are, or close over, foreign data?
      iow, `extern` is an effect in its own right
      this smells like the right "better is better" approach...
      ...but it seems like a painfully awkward world-split if everything has to keep type-track of whether it interacts with FFI
        another thing that would be like this is `trusted fn`
      (which split however _does_ reflect fundamentals in that you could do things with pure Ducks stuff that you just can't with foreign ones...)
      how much of the pain would be alleviated by effect polymorphism?
    also, how would we even express "a non-`extern` thing" (!extern) if effects are also row-based...?
      you can only express the presence, not the absence, of an effect...?

if rows are first-class and have many "consumers" - struct, enum, fn, Table, ..
  .. how do type components work?
  struct(type T, x: T)
    this is clear enough
  enum(type T, x: T)
    this makes no sense _whatsoever_
    but does that mean we need to forbid it, or merely that it's not useful?
  Table(type T, x: T)
    ???
    this would require equality-testing on types...? (to be a relation, rather than a multiset-thing)
  maybe there's a difference between "lowercase rows" and "uppercase rows" - only the latter can have type components?
    or rather, only uppercase rows can contain uppercase things
  is there anything besides structs and fns that _can_ handle uppercase rows?
    (various functor-combinators derived from them, like EitherF/BothF/ComposeF?)
    any other _primitives_?
    hashmap implementations of records -- same semantics, just optimized for large records, updates, and sharing?
      (can we do this phase transition automatically...?)
    the map-over-rows stuff for first-class pattern matching thingie also doesn't make sense over types...
      but conveniently, type components in enums don't make sense anyway
      an existential enum is an enum inside a dependent struct!
      so it seemingly makes sense to restrict map-over-rows to lowercase rows

"A functor is applicative if and only if its body is a provably pure module expression, otherwise it is generative. Likewise, sealing is weak (in the parlance of Dreyer et al. [1]) if and only if it seals a pure module, and strong otherwise. Moreover, there is a natural subtyping relation between pure and impure functor signatures." -- F-ing applicative functors
TODO look into the second two things!!

rewrite rules -> equality saturation? https://www.reddit.com/r/haskell/comments/2g6wsx/haskell_for_all_morte_an_intermediate_language/ckgevdt

strictness is the right default because:
 * more pervasive/familiar easier to think about etc etc
 * dual to call-by-value isn't actually Haskell-style call-by-need, but call-by-name, which is impractical
 * construction to add laziness to strict lang (() -> T, Thunk(T)) is much more obvious and easy to grok than dual in lazy lang (a :-> b)

first-class (built-in) Future/Event type (construct)? 
  "We then discuss the future construct, where the evaluation of its argument proceeds in parallel with the remainder of the computation." (~call-by-need)
  perhaps this is another kind of effect, `future fn()` or `parallel fn()` or so?
    (why) does it even need to be a separate type?
    "We can think of a thunk as a reference that we can write only once (the first time it is accessed) and henceforth will continue to be the same value."
      `fn()` ~ Thunk ~ IVar ~ Event ~ Future ??
    But the whole point of Events in FRPNow is that they are effectful! so we actually want `io fn()`?
      can we have a thing where starting the IO is effectful but waiting on its result is pure? (so an `io fn()` -> `pure fn()` transformerish)
        does this make any sense?? rethink when sober...
        --
        async :: IO a -> Now (Event a) 
        callback :: Now (Event a, a -> IO ()) 
        --
        new :: IO (IVar a)
        write :: IVar a -> a -> IO ()
        read :: IVar a -> a
        --
        these look very similar, but then how come there's no Event a -> a??
      this sounds like the whole point of `Event`s in FRPNow... also sounds like what IVars do (write-once)
    different names for (almost the?) same thing: Future, Event, Async, IVar, ...
    is C#'s async/await also the same thing??
      and what's the relationship between these and coroutines?
    is `async` an effect??
      if it is then what about Behaviors?
      after all they're both a bit function-like...
      behaviors non-blocking but return different results at different times, futures always return same results but may block
      are these perhaps dual in some way?
      is a behavior anything other than an impure but side-effect-free function?
        (but then what's the whole thing with push vs. pull...?)
      Event = Future, Behavior = Generator...? (Or rather EventStream = Generator? Probably.)
  also reminiscent of GHC's safe/unsafe foreign imports (~sync/async in FRPNow)
  what about Behaviors? Promises? EventStreams?

taking to row polymorphism lesson to scoped mutability:
  record/enum subtyping -> row polymorphism
  effect subtyping -> effect rows
  region subtyping -> region rows?
  or does this just fall out of effect rows?
